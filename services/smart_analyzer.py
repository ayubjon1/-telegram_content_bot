# services/smart_analyzer.py - –£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å ML
import re
import asyncio
import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json

logger = logging.getLogger(__name__)


class SmartContentAnalyzer:
    """–£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

    def __init__(self, db=None):
        self.db = db
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.engagement_model = None
        self.content_history = []
        self.performance_data = {}

        # –í–µ—Å–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        self.quality_weights = {
            'readability': 0.2,
            'uniqueness': 0.15,
            'engagement_potential': 0.25,
            'emotional_score': 0.15,
            'structure_score': 0.15,
            'trend_relevance': 0.1
        }

        logger.info("üß† –£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def analyze_content(self, text: str, context: Optional[Dict] = None) -> Dict:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏"""
        try:
            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            basic_metrics = self._calculate_basic_metrics(text)

            # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            quality_metrics = await self._calculate_quality_metrics(text)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            performance_prediction = await self._predict_performance(text, context)

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
            recommendations = self._generate_recommendations(
                basic_metrics, quality_metrics, performance_prediction
            )

            # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
            overall_score = self._calculate_overall_score(quality_metrics)

            # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            optimal_time = await self._predict_optimal_time(text, context)

            result = {
                'basic_metrics': basic_metrics,
                'quality_metrics': quality_metrics,
                'performance_prediction': performance_prediction,
                'recommendations': recommendations,
                'overall_score': overall_score,
                'optimal_publish_time': optimal_time,
                'analysis_timestamp': datetime.now().isoformat()
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            await self._save_analysis_result(text, result)

            return result

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return self._get_default_analysis()

    def _calculate_basic_metrics(self, text: str) -> Dict:
        """–†–∞—Å—á–µ—Ç –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ —Ç–µ–∫—Å—Ç–∞"""
        # –û—á–∏—â–∞–µ–º –æ—Ç HTML —Ç–µ–≥–æ–≤
        clean_text = re.sub(r'<[^>]+>', '', text)

        # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        words = clean_text.split()
        sentences = re.split(r'[.!?]+', clean_text)
        sentences = [s.strip() for s in sentences if s.strip()]

        # –≠–º–æ–¥–∑–∏
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+",
            flags=re.UNICODE
        )
        emojis = emoji_pattern.findall(text)

        # –•–µ—à—Ç–µ–≥–∏
        hashtags = re.findall(r'#\w+', text)

        # –í–æ–ø—Ä–æ—Å—ã
        questions = len(re.findall(r'\?', text))

        # –ü—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é
        cta_patterns = [
            r'–ø–æ–¥–ø–∏—Å', r'–ª–∞–π–∫', r'—Ä–µ–ø–æ—Å—Ç', r'–∫–æ–º–º–µ–Ω—Ç', r'–ø–æ–¥–µ–ª–∏—Å—å',
            r'–Ω–∞–∂–º–∏', r'–ø–µ—Ä–µ–π–¥–∏', r'—É–∑–Ω–∞–π', r'–ø–æ–ª—É—á–∏', r'—Ä–µ–≥–∏—Å—Ç—Ä'
        ]
        cta_count = sum(1 for pattern in cta_patterns if re.search(pattern, clean_text.lower()))

        return {
            'text_length': len(clean_text),
            'word_count': len(words),
            'sentence_count': len(sentences),
            'avg_word_length': np.mean([len(w) for w in words]) if words else 0,
            'avg_sentence_length': np.mean([len(s.split()) for s in sentences]) if sentences else 0,
            'emoji_count': len(emojis),
            'hashtag_count': len(hashtags),
            'question_count': questions,
            'cta_count': cta_count,
            'unique_words': len(set(words)),
            'lexical_diversity': len(set(words)) / len(words) if words else 0
        }

    async def _calculate_quality_metrics(self, text: str) -> Dict:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        # –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π Flesch Reading Ease –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
        basic_metrics = self._calculate_basic_metrics(text)

        # –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        if basic_metrics['word_count'] > 0 and basic_metrics['sentence_count'] > 0:
            readability = 206.835 - 1.015 * (basic_metrics['word_count'] / basic_metrics['sentence_count']) - 84.6 * (
                        basic_metrics['avg_word_length'] / 10)
            readability = max(0, min(100, readability))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ—Ç 0 –¥–æ 100
        else:
            readability = 50

        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞
        emotional_score = await self._calculate_emotional_score(text)

        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
        structure_score = self._calculate_structure_score(text)

        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏—Å—Ç–æ—Ä–∏–µ–π)
        uniqueness_score = await self._calculate_uniqueness(text)

        # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–æ–≤–ª–µ—á–µ–Ω–∏—è
        engagement_potential = self._calculate_engagement_potential(basic_metrics, emotional_score)

        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–¥–∞–º
        trend_relevance = await self._calculate_trend_relevance(text)

        return {
            'readability': readability,
            'emotional_score': emotional_score,
            'structure_score': structure_score,
            'uniqueness': uniqueness_score,
            'engagement_potential': engagement_potential,
            'trend_relevance': trend_relevance
        }

    async def _calculate_emotional_score(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
        positive_markers = [
            '!', 'üî•', 'üíØ', '‚úÖ', 'üëç', 'üòç', 'üöÄ', 'üí™', 'üéâ', '‚ù§Ô∏è',
            '–æ—Ç–ª–∏—á–Ω–æ', '—Å—É–ø–µ—Ä', '–∫—Ä—É—Ç–æ', '–ø–æ—Ç—Ä—è—Å–∞—é—â–µ', '–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ',
            '—É—Å–ø–µ—Ö', '–ø–æ–±–µ–¥–∞', '–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ', '–ø—Ä–æ—Ä—ã–≤', '—Ä–µ–≤–æ–ª—é—Ü–∏—è'
        ]

        negative_markers = [
            'üò¢', 'üò°', 'üëé', '‚ùå', '‚ö†Ô∏è', 'üö´',
            '–ø–ª–æ—Ö–æ', '—É–∂–∞—Å–Ω–æ', '–ø—Ä–æ–≤–∞–ª', '–ø—Ä–æ–±–ª–µ–º–∞', '–∫—Ä–∏–∑–∏—Å',
            '–æ–ø–∞—Å–Ω–æ', '—É–≥—Ä–æ–∑–∞', '—Ä–∏—Å–∫', '–ø–æ—Ç–µ—Ä—è', '–∫—Ä–∞—Ö'
        ]

        text_lower = text.lower()

        positive_count = sum(1 for marker in positive_markers if marker in text_lower)
        negative_count = sum(1 for marker in negative_markers if marker in text_lower)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç 0 –¥–æ 1 (0.5 - –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ)
        total_markers = positive_count + negative_count
        if total_markers == 0:
            return 0.5

        emotional_score = (positive_count - negative_count) / total_markers
        return (emotional_score + 1) / 2  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0-1

    def _calculate_structure_score(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        score = 0.0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        if re.search(r'^\d+\.', text, re.MULTILINE):  # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            score += 0.2
        if re.search(r'^[‚Ä¢¬∑\-\*]', text, re.MULTILINE):  # –ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            score += 0.2
        if re.search(r'<b>.*?</b>', text):  # –í—ã–¥–µ–ª–µ–Ω–∏–µ –∂–∏—Ä–Ω—ã–º
            score += 0.15
        if re.search(r'\n\n', text):  # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            score += 0.15
        if len(re.findall(r'[–ê-–ØA-Z][^.!?]*[.!?]', text)) > 3:  # –ù–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            score += 0.15
        if re.search(r'#\w+', text):  # –•–µ—à—Ç–µ–≥–∏
            score += 0.15

        return min(1.0, score)

    async def _calculate_uniqueness(self, text: str) -> float:
        """–†–∞—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ç–æ—Ä–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not self.content_history:
            return 1.0

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ –∏—Å—Ç–æ—Ä–∏–∏
        try:
            all_texts = self.content_history[-100:] + [text]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Ç–µ–∫—Å—Ç–æ–≤

            if len(all_texts) < 2:
                return 1.0

            # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
            vectors = self.vectorizer.fit_transform(all_texts)

            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
            current_vector = vectors[-1]
            history_vectors = vectors[:-1]

            similarities = cosine_similarity(current_vector, history_vectors).flatten()

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–µ–µ)
            max_similarity = np.max(similarities) if len(similarities) > 0 else 0

            uniqueness = 1.0 - max_similarity
            return max(0.0, min(1.0, uniqueness))

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
            return 0.8

    def _calculate_engagement_potential(self, basic_metrics: Dict, emotional_score: float) -> float:
        """–†–∞—Å—á–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –≤–æ–≤–ª–µ—á–µ–Ω–∏—è"""
        score = 0.0

        # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ (150-300 —Å–∏–º–≤–æ–ª–æ–≤)
        if 150 <= basic_metrics['text_length'] <= 300:
            score += 0.2
        elif 100 <= basic_metrics['text_length'] <= 400:
            score += 0.1

        # –≠–º–æ–¥–∑–∏ (2-5 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)
        if 2 <= basic_metrics['emoji_count'] <= 5:
            score += 0.15
        elif 1 <= basic_metrics['emoji_count'] <= 7:
            score += 0.08

        # –í–æ–ø—Ä–æ—Å—ã –≤–æ–≤–ª–µ–∫–∞—é—Ç
        if basic_metrics['question_count'] > 0:
            score += 0.15

        # –ü—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é
        if basic_metrics['cta_count'] > 0:
            score += 0.15

        # –•–µ—à—Ç–µ–≥–∏ (2-4 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)
        if 2 <= basic_metrics['hashtag_count'] <= 4:
            score += 0.1

        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        if emotional_score > 0.6 or emotional_score < 0.4:  # –ù–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ
            score += 0.15

        # –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
        if basic_metrics['lexical_diversity'] > 0.7:
            score += 0.1

        return min(1.0, score)

    async def _calculate_trend_relevance(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–Ω–¥–∞–º"""
        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏)
        trend_keywords = [
            '–∏–∏', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–Ω–µ–π—Ä–æ—Å–µ—Ç—å', 'chatgpt', 'ai',
            '2024', '2025', '–Ω–æ–≤—ã–π', '—Ä–µ–≤–æ–ª—é—Ü–∏—è', '–±—É–¥—É—â–µ–µ', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
            '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞', '–±–ª–æ–∫—á–µ–π–Ω', '–º–µ—Ç–∞–≤—Å–µ–ª–µ–Ω–Ω–∞—è', '—Ç—Ä–µ–Ω–¥'
        ]

        text_lower = text.lower()
        matches = sum(1 for keyword in trend_keywords if keyword in text_lower)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        relevance = min(1.0, matches / 5)  # 5 —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Å–ª–æ–≤ = –º–∞–∫—Å–∏–º—É–º
        return relevance

    async def _predict_performance(self, text: str, context: Optional[Dict] = None) -> Dict:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞"""
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        basic_metrics = self._calculate_basic_metrics(text)
        quality_metrics = await self._calculate_quality_metrics(text)

        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        base_views = 100
        base_likes = 5
        base_shares = 1

        # –ú–Ω–æ–∂–∏—Ç–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_multiplier = quality_metrics['engagement_potential'] * 2 + 0.5

        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
        if basic_metrics['emoji_count'] > 0:
            quality_multiplier *= 1.1
        if basic_metrics['question_count'] > 0:
            quality_multiplier *= 1.15
        if basic_metrics['hashtag_count'] > 0:
            quality_multiplier *= 1.05
        if quality_metrics['emotional_score'] > 0.7:
            quality_multiplier *= 1.2

        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
        if context:
            if context.get('time_of_day') in ['morning', 'evening']:
                quality_multiplier *= 1.2
            if context.get('day_of_week') in ['saturday', 'sunday']:
                quality_multiplier *= 0.8

        # –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predicted_views = int(base_views * quality_multiplier)
        predicted_likes = int(base_likes * quality_multiplier * 1.2)
        predicted_shares = int(base_shares * quality_multiplier * 0.8)
        predicted_engagement_rate = (predicted_likes + predicted_shares) / predicted_views * 100

        return {
            'predicted_views': predicted_views,
            'predicted_likes': predicted_likes,
            'predicted_shares': predicted_shares,
            'predicted_engagement_rate': round(predicted_engagement_rate, 2),
            'confidence': 0.75,  # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏
            'factors': {
                'quality_multiplier': round(quality_multiplier, 2),
                'main_drivers': self._get_main_performance_drivers(basic_metrics, quality_metrics)
            }
        }

    def _get_main_performance_drivers(self, basic_metrics: Dict, quality_metrics: Dict) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        drivers = []

        if quality_metrics['engagement_potential'] > 0.7:
            drivers.append("–í—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–æ–≤–ª–µ—á–µ–Ω–∏—è")
        if quality_metrics['emotional_score'] > 0.7:
            drivers.append("–°–∏–ª—å–Ω–∞—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞")
        if basic_metrics['question_count'] > 0:
            drivers.append("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç")
        if basic_metrics['emoji_count'] >= 2:
            drivers.append("–í–∏–∑—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        if quality_metrics['readability'] > 70:
            drivers.append("–û—Ç–ª–∏—á–Ω–∞—è —á–∏—Ç–∞–µ–º–æ—Å—Ç—å")
        if quality_metrics['trend_relevance'] > 0.5:
            drivers.append("–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–Ω–¥–∞–º")

        return drivers[:3]  # –¢–æ–ø-3 —Ñ–∞–∫—Ç–æ—Ä–∞

    def _generate_recommendations(self, basic_metrics: Dict, quality_metrics: Dict,
                                  performance_prediction: Dict) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        recommendations = []

        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã
        if basic_metrics['text_length'] < 100:
            recommendations.append({
                'type': 'length',
                'priority': 'high',
                'message': '–£–≤–µ–ª–∏—á—å—Ç–µ –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–æ 150-300 —Å–∏–º–≤–æ–ª–æ–≤',
                'impact': '+20% –∫ –æ—Ö–≤–∞—Ç—É'
            })
        elif basic_metrics['text_length'] > 500:
            recommendations.append({
                'type': 'length',
                'priority': 'medium',
                'message': '–°–æ–∫—Ä–∞—Ç–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏',
                'impact': '+15% –∫ –≤–æ–≤–ª–µ—á–µ–Ω–∏—é'
            })

        # –≠–º–æ–¥–∑–∏
        if basic_metrics['emoji_count'] == 0:
            recommendations.append({
                'type': 'emoji',
                'priority': 'high',
                'message': '–î–æ–±–∞–≤—å—Ç–µ 2-3 —ç–º–æ–¥–∑–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏',
                'impact': '+25% –∫ –ª–∞–π–∫–∞–º'
            })
        elif basic_metrics['emoji_count'] > 7:
            recommendations.append({
                'type': 'emoji',
                'priority': 'low',
                'message': '–£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–æ–¥–∑–∏',
                'impact': '+10% –∫ –¥–æ–≤–µ—Ä–∏—é'
            })

        # –í–æ–ø—Ä–æ—Å—ã
        if basic_metrics['question_count'] == 0 and quality_metrics['engagement_potential'] < 0.5:
            recommendations.append({
                'type': 'question',
                'priority': 'medium',
                'message': '–î–æ–±–∞–≤—å—Ç–µ –≤–æ–ø—Ä–æ—Å –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –≤–æ–≤–ª–µ—á–µ–Ω–∏—è',
                'impact': '+30% –∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º'
            })

        # –•–µ—à—Ç–µ–≥–∏
        if basic_metrics['hashtag_count'] == 0:
            recommendations.append({
                'type': 'hashtag',
                'priority': 'medium',
                'message': '–î–æ–±–∞–≤—å—Ç–µ 2-3 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ö–µ—à—Ç–µ–≥–∞',
                'impact': '+15% –∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é'
            })

        # –ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é
        if basic_metrics['cta_count'] == 0:
            recommendations.append({
                'type': 'cta',
                'priority': 'medium',
                'message': '–î–æ–±–∞–≤—å—Ç–µ –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é',
                'impact': '+40% –∫ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏'
            })

        # –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å
        if quality_metrics['readability'] < 60:
            recommendations.append({
                'type': 'readability',
                'priority': 'high',
                'message': '–£–ø—Ä–æ—Å—Ç–∏—Ç–µ —Ç–µ–∫—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è',
                'impact': '+20% –∫ –¥–æ—á–∏—Ç—ã–≤–∞–µ–º–æ—Å—Ç–∏'
            })

        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        if 0.4 <= quality_metrics['emotional_score'] <= 0.6:
            recommendations.append({
                'type': 'emotion',
                'priority': 'low',
                'message': '–î–æ–±–∞–≤—å—Ç–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É',
                'impact': '+15% –∫ –≤–æ–≤–ª–µ—á–µ–Ω–∏—é'
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        priority_order = {'high': 0, 'medium': 1, 'low': 2}
        recommendations.sort(key=lambda x: priority_order[x['priority']])

        return recommendations[:5]  # –¢–æ–ø-5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

    def _calculate_overall_score(self, quality_metrics: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
        score = 0.0

        for metric, weight in self.quality_weights.items():
            if metric in quality_metrics:
                score += quality_metrics[metric] * weight

        return round(score, 2)

    async def _predict_optimal_time(self, text: str, context: Optional[Dict] = None) -> Dict:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_type = self._detect_content_type(text)

        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø–æ —Ç–∏–ø—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        optimal_times = {
            'news': ['09:00', '12:00', '18:00'],
            'educational': ['10:00', '15:00', '20:00'],
            'entertainment': ['19:00', '21:00', '22:00'],
            'business': ['08:00', '11:00', '14:00'],
            'general': ['09:00', '15:00', '21:00']
        }

        # –î–Ω–∏ –Ω–µ–¥–µ–ª–∏
        best_days = {
            'news': ['monday', 'tuesday', 'wednesday', 'thursday', 'friday'],
            'educational': ['tuesday', 'wednesday', 'thursday'],
            'entertainment': ['friday', 'saturday', 'sunday'],
            'business': ['tuesday', 'wednesday', 'thursday'],
            'general': ['tuesday', 'wednesday', 'thursday', 'friday']
        }

        times = optimal_times.get(content_type, optimal_times['general'])
        days = best_days.get(content_type, best_days['general'])

        return {
            'recommended_times': times,
            'best_days': days,
            'content_type': content_type,
            'reasoning': f"–î–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ç–∏–ø–∞ '{content_type}' –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {times[0]}"
        }

    def _detect_content_type(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        text_lower = text.lower()

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
        news_keywords = ['—Å–µ–≥–æ–¥–Ω—è', '–≤—á–µ—Ä–∞', '–ø—Ä–æ–∏–∑–æ—à–ª–æ', '–∑–∞—è–≤–∏–ª', '—Å–æ–æ–±—â–∞–µ—Ç', '–Ω–æ–≤–æ—Å—Ç—å']
        educational_keywords = ['—É–∑–Ω–∞–π—Ç–µ', '–∫–∞–∫', '–ø–æ—á–µ–º—É', '—Å–æ–≤–µ—Ç', '–≥–∞–π–¥', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è']
        entertainment_keywords = ['—Å–º–µ—à–Ω–æ', '–ø—Ä–∏–∫–æ–ª', '–º–µ–º', '–≤–∏–¥–µ–æ', '—Ñ–æ—Ç–æ', '–ª–æ–ª']
        business_keywords = ['–±–∏–∑–Ω–µ—Å', '–¥–µ–Ω—å–≥–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '—Å—Ç–∞—Ä—Ç–∞–ø', '–¥–æ—Ö–æ–¥', '–ø—Ä–æ–¥–∞–∂–∏']

        # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        scores = {
            'news': sum(1 for kw in news_keywords if kw in text_lower),
            'educational': sum(1 for kw in educational_keywords if kw in text_lower),
            'entertainment': sum(1 for kw in entertainment_keywords if kw in text_lower),
            'business': sum(1 for kw in business_keywords if kw in text_lower)
        }

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å—á–µ—Ç–æ–º
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)

        return 'general'

    async def _save_analysis_result(self, text: str, result: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.content_history.append(text)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.content_history) > 1000:
            self.content_history = self.content_history[-1000:]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if self.db:
            try:
                await self.db.set_setting(
                    f'content_analysis_{datetime.now().timestamp()}',
                    json.dumps(result)
                )
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞: {e}")

    def _get_default_analysis(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        return {
            'basic_metrics': {
                'text_length': 0,
                'word_count': 0,
                'emoji_count': 0,
                'hashtag_count': 0
            },
            'quality_metrics': {
                'readability': 50,
                'engagement_potential': 0.5,
                'uniqueness': 0.8
            },
            'performance_prediction': {
                'predicted_views': 100,
                'predicted_likes': 5,
                'predicted_engagement_rate': 5.0
            },
            'recommendations': [{
                'type': 'error',
                'priority': 'high',
                'message': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç',
                'impact': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            }],
            'overall_score': 0.5,
            'error': True
        }

    async def analyze_batch(self, texts: List[str]) -> List[Dict]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤"""
        results = []

        for text in texts:
            result = await self.analyze_content(text)
            results.append(result)
            await asyncio.sleep(0.1)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞

        return results

    async def compare_contents(self, text1: str, text2: str) -> Dict:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        analysis1 = await self.analyze_content(text1)
        analysis2 = await self.analyze_content(text2)

        comparison = {
            'text1_score': analysis1['overall_score'],
            'text2_score': analysis2['overall_score'],
            'winner': 1 if analysis1['overall_score'] > analysis2['overall_score'] else 2,
            'score_difference': abs(analysis1['overall_score'] - analysis2['overall_score']),
            'key_differences': self._find_key_differences(analysis1, analysis2)
        }

        return comparison

    def _find_key_differences(self, analysis1: Dict, analysis2: Dict) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –∞–Ω–∞–ª–∏–∑–∞–º–∏"""
        differences = []

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics_to_compare = [
            ('readability', '–ß–∏—Ç–∞–µ–º–æ—Å—Ç—å'),
            ('engagement_potential', '–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–æ–≤–ª–µ—á–µ–Ω–∏—è'),
            ('emotional_score', '–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å'),
            ('uniqueness', '–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å')
        ]

        for metric_key, metric_name in metrics_to_compare:
            val1 = analysis1['quality_metrics'].get(metric_key, 0)
            val2 = analysis2['quality_metrics'].get(metric_key, 0)

            diff = abs(val1 - val2)
            if diff > 0.2:  # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞
                if val1 > val2:
                    differences.append(f"–¢–µ–∫—Å—Ç 1 –∏–º–µ–µ—Ç –ª—É—á—à—É—é {metric_name.lower()} (+{diff:.1%})")
                else:
                    differences.append(f"–¢–µ–∫—Å—Ç 2 –∏–º–µ–µ—Ç –ª—É—á—à—É—é {metric_name.lower()} (+{diff:.1%})")

        return differences[:3]  # –¢–æ–ø-3 —Ä–∞–∑–ª–∏—á–∏—è